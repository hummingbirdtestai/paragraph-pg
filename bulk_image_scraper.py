import os
import requests
from supabase import create_client, Client
from dotenv import load_dotenv

load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

BUCKET = "medical-images"

def download_file(url: str) -> bytes:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        "Referer": "https://google.com"
    }
    r = requests.get(url, headers=headers, timeout=20)
    r.raise_for_status()
    return r.content

def upload_to_supabase(file_bytes: bytes, filename: str) -> str:
    path = f"feed_images/{filename}"

    res = supabase.storage.from_(BUCKET).upload(
        path,
        file_bytes,
        {"content-type": "application/octet-stream", "upsert": True}
    )

    if isinstance(res, dict) and "error" in res:
        raise Exception(res["error"])

    public_url = supabase.storage.from_(BUCKET).get_public_url(path)
    return public_url["publicUrl"]

def process_all():
    print("\nğŸ” Fetching rows where:")
    print("â¡ image_url IS NOT NULL")
    print("â¡ image_url_supabase IS NULL")

    rows = (
        supabase.table("feed_posts")
        .select("*")
        .neq("image_url", None)
        .is_("image_url_supabase", None)
        .execute()
    )

    if not rows.data:
        print("ğŸ‰ Nothing left to process!")
        return

    print(f"ğŸ“Œ Found {len(rows.data)} images to process")

    for row in rows.data:
        try:
            print(f"\nâ¡ Processing ID: {row['id']}")
            url = row["image_url"]

            # ğŸ”¥ Skip invalid data types (bool, dict, list, null)
            if not isinstance(url, str):
                print(f"â›” Skipping invalid URL type: {url} ({type(url)})")
                continue

            url = url.strip()
            if url == "" or url.lower() in ["none", "null"]:
                print(f"â›” Skipping empty/null URL: {url}")
                continue

            # ğŸ”¥ Skip ResearchGate (always 403)
            if "researchgate" in url:
                print(f"â›” ResearchGate blocks scraping â†’ skipping: {url}")
                continue

            if not url.startswith("http"):
                print(f"â›” Skipping bad URL: {url}")
                continue

            # Step 1: Download
            img_bytes = download_file(url)

            # Step 2: Store as id.jpg
            filename = f"{row['id']}.jpg"

            # Step 3: Upload to Supabase
            public_url = upload_to_supabase(img_bytes, filename)

            # Step 4: Update table
            supabase.table("feed_posts").update({
                "image_url_supabase": public_url
            }).eq("id", row["id"]).execute()

            print(f"âœ… Uploaded â†’ {public_url}")

        except requests.exceptions.HTTPError as e:
            print(f"âŒ HTTP error for {row['id']}: {e}")
        except Exception as e:
            print(f"âŒ Error processing {row['id']}: {e}")

if __name__ == "__main__":
    process_all()
